# ViT-Beans-Classifier

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ronbragaglia/ViT-Beans-Classifier/blob/main/ViT_Beans_Classifier.ipynb) <!-- *** IMPORTANTE: Substitua 'ViT_Beans_Classifier.ipynb' pelo nome real do seu arquivo .ipynb no GitHub *** -->
[![Python Version](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Reposit√≥rio com um exemplo completo de fine-tuning do modelo Vision Transformer (ViT) para classifica√ß√£o de imagens de folhas de feij√£o, identificando diferentes doen√ßas ou folhas saud√°veis. Este projeto utiliza PyTorch e o ecossistema Hugging Face (Transformers, Datasets, Evaluate).

Desenvolvido por [Ronbragaglia](https://github.com/Ronbragaglia).

## üéØ Objetivo

O objetivo deste projeto √© demonstrar um pipeline robusto para treinar um modelo de vis√£o computacional de √∫ltima gera√ß√£o (ViT) em um dataset espec√≠fico ('beans' da Hugging Face), alcan√ßando alta acur√°cia na classifica√ß√£o das imagens, mesmo em ambientes com recursos limitados (como CPU no Colab).

## ‚ú® Principais Caracter√≠sticas

*   **Modelo:** Fine-tuning do Vision Transformer pr√©-treinado (`google/vit-base-patch16-224-in21k`).
*   **Framework:** PyTorch.
*   **Bibliotecas:** Hugging Face `transformers`, `datasets`, `evaluate`.
*   **Pr√©-processamento:** Uso de `torchvision.transforms` (v2/v1) para data augmentation e normaliza√ß√£o, garantindo estabilidade.
*   **Loop de Treinamento Customizado:** Controle expl√≠cito sobre o treinamento, avalia√ß√£o, otimiza√ß√£o e atualiza√ß√£o do learning rate.
*   **T√©cnicas Avan√ßadas:**
    *   Otimizador AdamW com Weight Decay.
    *   Learning Rate Scheduler (Cosine Annealing com Warmup).
    *   Label Smoothing na fun√ß√£o de perda CrossEntropy.
    *   Gradient Clipping para estabiliza√ß√£o.
    *   Mixed Precision (AMP) via `torch.cuda.amp` (configurado para ativar se GPU dispon√≠vel, desativado na execu√ß√£o em CPU).
*   **Monitoramento:** Integra√ß√£o com [Weights & Biases (W&B)](https://wandb.ai/) para logar m√©tricas, hiperpar√¢metros, matriz de confus√£o e salvar o melhor modelo como artefato.
*   **Avalia√ß√£o:** C√°lculo de acur√°cia e gera√ß√£o de matriz de confus√£o no conjunto de teste.
*   **Reprodutibilidade:** Uso de seed para inicializa√ß√£o de pesos e processos.
*   **C√≥digo:** Implementado em um notebook Google Colab (`.ipynb`) com tratamento de erros e coment√°rios detalhados.

## üå± Dataset

Utilizamos o dataset [`beans`](https://huggingface.co/datasets/beans) dispon√≠vel na Hugging Face Datasets Hub. Ele cont√©m imagens de folhas de feij√£o classificadas em tr√™s categorias:
*   `angular_leaf_spot` (Mancha-angular)
*   `bean_rust` (Ferrugem)
*   `healthy` (Saud√°vel)

## üìà Resultados

Ap√≥s o fine-tuning por 4 √©pocas (executado em CPU no ambiente Colab), o modelo alcan√ßou:

*   **Melhor Acur√°cia de Valida√ß√£o:** **99.25%** (atingida na √âpoca 3)
*   **Acur√°cia Final no Conjunto de Teste:** **96.88%**

A matriz de confus√£o gerada (e logada no W&B) confirma o excelente desempenho do modelo, com poucos erros de classifica√ß√£o no conjunto de teste.

*(Nota: Os avisos `FutureWarning: torch.cuda.amp.autocast(...) is deprecated` foram observados, mas como a execu√ß√£o foi em CPU, o `autocast` estava desabilitado e o aviso n√£o impactou o resultado. Para execu√ß√µes futuras em GPU, a sintaxe pode ser atualizada para `torch.amp.autocast('cuda', ...)`)*

## üõ†Ô∏è Tecnologias Utilizadas

*   Python 3.11+
*   PyTorch
*   Hugging Face Transformers
*   Hugging Face Datasets
*   Hugging Face Evaluate
*   Torchvision
*   Weights & Biases (wandb)
*   Scikit-learn
*   NumPy
*   Matplotlib
*   Tqdm
*   Google Colab

## Resultados obtidos:
![image](https://github.com/user-attachments/assets/0d999558-f3a2-48fc-93f8-01aeff6d04d5)

![image](https://github.com/user-attachments/assets/182046a3-0131-4d7f-a7fe-7d8c4a35a13d)

![image](https://github.com/user-attachments/assets/a5349ecd-bd42-4d4e-b850-ac364a008a8a)

![image](https://github.com/user-attachments/assets/4a02af4b-d985-4e11-8303-7e4778efc9e6)

![image](https://github.com/user-attachments/assets/9dec4d45-6ee0-45b5-8cfb-41b7126fc70a)



## ‚öôÔ∏è Configura√ß√£o e Instala√ß√£o

1.  **Clone o Reposit√≥rio:**
    ```bash
    git clone https://github.com/Ronbragaglia/ViT-Beans-Classifier.git
    cd ViT-Beans-Classifier
    ```

2.  **Crie um Ambiente Virtual (Recomendado):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # Linux/macOS
    # venv\Scripts\activate  # Windows
    ```

3.  **Instale as Depend√™ncias:**
    Crie um arquivo `requirements.txt` com o seguinte conte√∫do:
    ```txt
    transformers
    datasets
    evaluate
    accelerate
    torch
    torchvision
    wandb
    scikit-learn
    tqdm
    matplotlib
    numpy
    ```
    E ent√£o instale:
    ```bash
    pip install -r requirements.txt
    ```

## ‚ñ∂Ô∏è Como Usar

A maneira mais f√°cil de executar este projeto √© usando o Google Colab:

1.  **Abra no Colab:** Clique no bot√£o "Open In Colab" no topo deste README ou acesse o link diretamente: [`https://colab.research.google.com/github/Ronbragaglia/ViT-Beans-Classifier/blob/main/ViT_Beans_Classifier.ipynb`](https://colab.research.google.com/github/Ronbragaglia/ViT-Beans-Classifier/blob/main/ViT_Beans_Classifier.ipynb) <!-- *** ATUALIZE ESTE LINK com o nome correto do seu notebook .ipynb *** -->
2.  **Configure o Ambiente de Execu√ß√£o:**
    *   V√° em `Ambiente de execution` -> `Alterar o tipo de ambiente de execution`.
    *   Selecione `GPU` como acelerador de hardware (altamente recomendado para velocidade) ou `None` para CPU.
    *   Clique em `Salvar`.
3.  **Execute as C√©lulas:** Execute as c√©lulas do notebook sequencialmente (`Shift + Enter` ou bot√£o de Play).
4.  **Weights & Biases Login:** A C√©lula 1 tentar√° fazer login no W&B. Siga as instru√ß√µes no output:
    *   Geralmente, digite `2` para usar uma conta existente.
    *   **SE** solicitado, cole sua chave de API do W&B (encontrada em [wandb.ai/authorize](https://wandb.ai/authorize)).
    *   Se preferir n√£o usar o W&B, pode escolher a op√ß√£o `3` ou deixar o login falhar (o script continuar√° desabilitando o W&B).
5.  **Acompanhe o Treinamento:** O progresso ser√° exibido no notebook (barras de progresso, logs de √©poca). Se o W&B estiver ativo, acesse o link da run fornecido no output para ver gr√°ficos e logs detalhados.
6.  **Resultados:** Ao final, o modelo treinado ser√° avaliado no conjunto de teste, a matriz de confus√£o ser√° exibida/salva, e um exemplo de infer√™ncia ser√° mostrado. O melhor modelo ser√° salvo no diret√≥rio `./vit-beans-torchvision-run/best_model_pretrained`.

## üìÇ Estrutura do Projeto
.
‚îú‚îÄ‚îÄ ViT_Beans_Classifier.ipynb # <-- CONFIRME/AJUSTE O NOME DO ARQUIVO .ipynb
‚îú‚îÄ‚îÄ requirements.txt # Depend√™ncias Python
‚îú‚îÄ‚îÄ README.md # Este arquivo
‚îî‚îÄ‚îÄ (Opcional) LICENSE # Arquivo de licen√ßa (ex: MIT)
‚îî‚îÄ‚îÄ (Gerado pela execu√ß√£o) vit-beans-torchvision-run/ # Diret√≥rio com modelo salvo, matriz, etc.


## üìú Licen√ßa

Este projeto est√° licenciado sob a Licen√ßa MIT.

## üôè Agradecimentos

*   √Ä equipe da [Hugging Face](https://huggingface.co/) pelas bibliotecas `transformers`, `datasets`, e `evaluate`.
*   √Ä equipe do [PyTorch](https://pytorch.org/) e [Torchvision](https://pytorch.org/vision/stable/index.html).
*   Ao Google pela pesquisa e disponibiliza√ß√£o do modelo Vision Transformer (ViT).
*   Aos criadores do dataset `beans`.
*   √Ä equipe do [Weights & Biases](https://wandb.ai/) pela ferramenta de monitoramento.

*   
