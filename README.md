# ViT-Beans-Classifier (ou o nome que voc√™ escolher)

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/[SEU-USUARIO-GITHUB]/[NOME-DO-REPO]/blob/main/NomeDoSeuNotebook.ipynb) <!-- Substitua pelo link direto para o notebook no GitHub -->
[![Python Version](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Reposit√≥rio com um exemplo completo de fine-tuning do modelo Vision Transformer (ViT) para classifica√ß√£o de imagens de folhas de feij√£o, identificando diferentes doen√ßas ou folhas saud√°veis. Este projeto utiliza PyTorch e o ecossistema Hugging Face (Transformers, Datasets, Evaluate).

## üéØ Objetivo

O objetivo deste projeto √© demonstrar um pipeline robusto para treinar um modelo de vis√£o computacional de √∫ltima gera√ß√£o (ViT) em um dataset espec√≠fico ('beans' da Hugging Face), alcan√ßando alta acur√°cia na classifica√ß√£o das imagens.

## ‚ú® Principais Caracter√≠sticas

*   **Modelo:** Fine-tuning do Vision Transformer pr√©-treinado (`google/vit-base-patch16-224-in21k`).
*   **Framework:** PyTorch.
*   **Bibliotecas:** Hugging Face `transformers`, `datasets`, `evaluate`.
*   **Pr√©-processamento:** Uso de `torchvision.transforms` para data augmentation e normaliza√ß√£o.
*   **Loop de Treinamento Customizado:** Controle expl√≠cito sobre o treinamento, avalia√ß√£o, otimiza√ß√£o e atualiza√ß√£o do learning rate.
*   **T√©cnicas Avan√ßadas:**
    *   Otimizador AdamW com Weight Decay.
    *   Learning Rate Scheduler (Cosine Annealing com Warmup).
    *   Label Smoothing na fun√ß√£o de perda CrossEntropy.
    *   Gradient Clipping para estabiliza√ß√£o.
    *   Mixed Precision (AMP) via `torch.cuda.amp` (ativado se GPU dispon√≠vel).
*   **Monitoramento:** Integra√ß√£o com [Weights & Biases (W&B)](https://wandb.ai/) para logar m√©tricas, hiperpar√¢metros, matriz de confus√£o e salvar o melhor modelo como artefato.
*   **Avalia√ß√£o:** C√°lculo de acur√°cia e gera√ß√£o de matriz de confus√£o no conjunto de teste.
*   **Reprodutibilidade:** Uso de seed para inicializa√ß√£o de pesos e divis√µes de dados.
*   **C√≥digo:** Implementado em um notebook Google Colab (`.ipynb`) para f√°cil execu√ß√£o e experimenta√ß√£o.

## üå± Dataset

Utilizamos o dataset [`beans`](https://huggingface.co/datasets/beans) dispon√≠vel na Hugging Face Datasets Hub. Ele cont√©m imagens de folhas de feij√£o classificadas em tr√™s categorias:
*   `angular_leaf_spot` (Mancha-angular)
*   `bean_rust` (Ferrugem)
*   `healthy` (Saud√°vel)

## üìà Resultados

Ap√≥s o fine-tuning por 4 √©pocas (conforme o log fornecido), o modelo alcan√ßou:

*   **Melhor Acur√°cia de Valida√ß√£o:** 99.25%
*   **Acur√°cia Final no Conjunto de Teste:** 96.88%

A matriz de confus√£o gerada (e logada no W&B) mostra um excelente desempenho na distin√ß√£o entre as classes no conjunto de teste.

*(Opcional: Voc√™ pode incluir a imagem da matriz de confus√£o aqui)*
```markdown
![Matriz de Confus√£o](caminho/para/sua/imagem/confusion_matrix_test.png) 
<!-- Se voc√™ fizer upload da imagem para o repo, ajuste o caminho -->

üõ†Ô∏è Tecnologias Utilizadas
Python 3.11+

PyTorch

Hugging Face Transformers

Hugging Face Datasets

Hugging Face Evaluate

Torchvision

Weights & Biases (wandb)

Scikit-learn (para matriz de confus√£o)

NumPy

Matplotlib


git clone https://github.com/[SEU-USUARIO-GITHUB]/[NOME-DO-REPO].git
cd [NOME-DO-REPO]

python -m venv venv
source venv/bin/activate  # Linux/macOS
# venv\Scripts\activate  # Windows

transformers
datasets
evaluate
accelerate
torch
torchvision
wandb
scikit-learn
tqdm
matplotlib
numpy
# Adicione outras depend√™ncias se necess√°rio

pip install -r requirements.txt

‚ñ∂Ô∏è Como Usar
A maneira mais f√°cil de executar este projeto √© usando o Google Colab:

Abra no Colab: Clique no bot√£o "Open In Colab" no topo deste README ou acesse o link diretamente: https://colab.research.google.com/github/[SEU-USUARIO-GITHUB]/[NOME-DO-REPO]/blob/main/NomeDoSeuNotebook.ipynb <!-- **ATUALIZE ESTE LINK** -->

Configure o Ambiente de Execu√ß√£o:

V√° em Ambiente de execu√ß√£o -> Alterar o tipo de ambiente de execu√ß√£o.

Selecione GPU como acelerador de hardware (recomendado para velocidade).

Clique em Salvar.

Execute as C√©lulas: Execute as c√©lulas do notebook sequencialmente, uma por uma.

Weights & Biases Login: A primeira c√©lula pedir√° para voc√™ fazer login no W&B. Siga as instru√ß√µes:

Escolha a op√ß√£o 2 (Use an existing W&B account).

SE for solicitado, cole sua chave de API do W&B (encontrada em wandb.ai/authorize).

Se preferir n√£o usar o W&B, pode escolher a op√ß√£o 3.

Acompanhe o Treinamento: O progresso ser√° exibido no notebook e, se o W&B estiver ativo, no seu dashboard do W&B.

Resultados: Ao final, o modelo treinado ser√° avaliado no conjunto de teste, a matriz de confus√£o ser√° exibida/salva, e um exemplo de infer√™ncia ser√° mostrado. O melhor modelo ser√° salvo no diret√≥rio especificado (./vit-beans-torchvision-run/best_model_pretrained por padr√£o).

.
‚îú‚îÄ‚îÄ NomeDoSeuNotebook.ipynb     # O notebook principal com todo o c√≥digo
‚îú‚îÄ‚îÄ requirements.txt            # Depend√™ncias Python
‚îú‚îÄ‚îÄ README.md                   # Este arquivo
‚îî‚îÄ‚îÄ (Opcional) LICENSE          # Arquivo de licen√ßa (ex: MIT)
‚îî‚îÄ‚îÄ (Gerado pela execu√ß√£o) vit-beans-torchvision-run/ # Diret√≥rio com modelo salvo, matriz, etc.

üìú Licen√ßa
Este projeto est√° licenciado sob a Licen√ßa MIT. Veja o arquivo LICENSE para mais detalhes.
(Sugest√£o: Crie um arquivo LICENSE na raiz com o texto da licen√ßa MIT).

üôè Agradecimentos
√Ä equipe da Hugging Face pelas excelentes bibliotecas transformers, datasets, e evaluate.

Ao Google pela pesquisa e disponibiliza√ß√£o do modelo Vision Transformer (ViT).

Aos criadores do dataset beans.

√Ä equipe do Weights & Biases pela ferramenta de monitoramento.
